2018/09/13 15:54:23 INFO  [SparkContext] - Running Spark version 2.2.0
2018/09/13 15:54:24 DEBUG [MutableMetricsFactory] - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
2018/09/13 15:54:24 DEBUG [MutableMetricsFactory] - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
2018/09/13 15:54:24 DEBUG [MutableMetricsFactory] - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about=, sampleName=Ops, type=DEFAULT, value=[GetGroups], valueName=Time)
2018/09/13 15:54:24 DEBUG [MetricsSystemImpl] - UgiMetrics, User and group related metrics
2018/09/13 15:54:24 DEBUG [KerberosName] - Kerberos krb5 configuration not found, setting default realm to empty
2018/09/13 15:54:24 DEBUG [Groups] -  Creating new Groups object
2018/09/13 15:54:24 DEBUG [NativeCodeLoader] - Trying to load the custom-built native-hadoop library...
2018/09/13 15:54:24 DEBUG [NativeCodeLoader] - Loaded the native-hadoop library
2018/09/13 15:54:24 DEBUG [JniBasedUnixGroupsMapping] - Using JniBasedUnixGroupsMapping for Group resolution
2018/09/13 15:54:24 DEBUG [JniBasedUnixGroupsMappingWithFallback] - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
2018/09/13 15:54:24 DEBUG [Groups] - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
2018/09/13 15:54:24 DEBUG [UserGroupInformation] - hadoop login
2018/09/13 15:54:24 DEBUG [UserGroupInformation] - hadoop login commit
2018/09/13 15:54:24 DEBUG [UserGroupInformation] - using local user:NTUserPrincipal: Administrator
2018/09/13 15:54:24 DEBUG [UserGroupInformation] - Using user: "NTUserPrincipal: Administrator" with name Administrator
2018/09/13 15:54:24 DEBUG [UserGroupInformation] - User entry: "Administrator"
2018/09/13 15:54:24 DEBUG [UserGroupInformation] - UGI loginUser:Administrator (auth:SIMPLE)
2018/09/13 15:54:24 INFO  [SparkContext] - Submitted application: RiskEvaluation
2018/09/13 15:54:24 INFO  [SecurityManager] - Changing view acls to: Administrator
2018/09/13 15:54:24 INFO  [SecurityManager] - Changing modify acls to: Administrator
2018/09/13 15:54:24 INFO  [SecurityManager] - Changing view acls groups to: 
2018/09/13 15:54:24 INFO  [SecurityManager] - Changing modify acls groups to: 
2018/09/13 15:54:24 INFO  [SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018/09/13 15:54:24 DEBUG [SecurityManager] - Created SSL options for fs: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
2018/09/13 15:54:24 DEBUG [InternalLoggerFactory] - Using SLF4J as the default logging framework
2018/09/13 15:54:24 DEBUG [PlatformDependent] - -Dio.netty.noUnsafe: false
2018/09/13 15:54:24 DEBUG [PlatformDependent0] - java.nio.Buffer.address: available
2018/09/13 15:54:24 DEBUG [PlatformDependent0] - sun.misc.Unsafe.theUnsafe: available
2018/09/13 15:54:24 DEBUG [PlatformDependent0] - sun.misc.Unsafe.copyMemory: available
2018/09/13 15:54:24 DEBUG [PlatformDependent0] - direct buffer constructor: available
2018/09/13 15:54:24 DEBUG [PlatformDependent0] - java.nio.Bits.unaligned: available, true
2018/09/13 15:54:24 DEBUG [PlatformDependent0] - java.nio.DirectByteBuffer.<init>(long, int): available
2018/09/13 15:54:24 DEBUG [Cleaner0] - java.nio.ByteBuffer.cleaner(): available
2018/09/13 15:54:24 DEBUG [PlatformDependent] - Platform: Windows
2018/09/13 15:54:24 DEBUG [PlatformDependent] - Java version: 8
2018/09/13 15:54:24 DEBUG [PlatformDependent] - sun.misc.Unsafe: available
2018/09/13 15:54:24 DEBUG [PlatformDependent] - -Dio.netty.noJavassist: false
2018/09/13 15:54:24 DEBUG [PlatformDependent] - Javassist: available
2018/09/13 15:54:24 DEBUG [PlatformDependent] - -Dio.netty.tmpdir: C:\Users\Administrator\AppData\Local\Temp (java.io.tmpdir)
2018/09/13 15:54:24 DEBUG [PlatformDependent] - -Dio.netty.bitMode: 64 (sun.arch.data.model)
2018/09/13 15:54:24 DEBUG [PlatformDependent] - -Dio.netty.noPreferDirect: false
2018/09/13 15:54:24 DEBUG [PlatformDependent] - io.netty.maxDirectMemory: 1883242496 bytes
2018/09/13 15:54:24 DEBUG [JavassistTypeParameterMatcherGenerator] - Generated: io.netty.util.internal.__matchers__.org.apache.spark.network.protocol.MessageMatcher
2018/09/13 15:54:24 DEBUG [JavassistTypeParameterMatcherGenerator] - Generated: io.netty.util.internal.__matchers__.io.netty.buffer.ByteBufMatcher
2018/09/13 15:54:24 DEBUG [MultithreadEventLoopGroup] - -Dio.netty.eventLoopThreads: 16
2018/09/13 15:54:24 DEBUG [NioEventLoop] - -Dio.netty.noKeySetOptimization: false
2018/09/13 15:54:24 DEBUG [NioEventLoop] - -Dio.netty.selectorAutoRebuildThreshold: 512
2018/09/13 15:54:24 DEBUG [PlatformDependent] - org.jctools-core.MpscChunkedArrayQueue: available
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.numHeapArenas: 16
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.numDirectArenas: 16
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.pageSize: 8192
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.maxOrder: 11
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.chunkSize: 16777216
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.tinyCacheSize: 512
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.smallCacheSize: 256
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.normalCacheSize: 64
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.cacheTrimInterval: 8192
2018/09/13 15:54:24 DEBUG [PooledByteBufAllocator] - -Dio.netty.allocator.useCacheForAllThreads
2018/09/13 15:54:25 DEBUG [ThreadLocalRandom] - -Dio.netty.initialSeedUniquifier: 0xd8ccbfc05a9f7ad6 (took 0 ms)
2018/09/13 15:54:25 DEBUG [ByteBufUtil] - -Dio.netty.allocator.type: unpooled
2018/09/13 15:54:25 DEBUG [ByteBufUtil] - -Dio.netty.threadLocalDirectBufferSize: 65536
2018/09/13 15:54:25 DEBUG [ByteBufUtil] - -Dio.netty.maxThreadLocalCharBufferSize: 16384
2018/09/13 15:54:25 DEBUG [NetUtil] - Loopback interface: lo (Software Loopback Interface 1, 127.0.0.1)
2018/09/13 15:54:25 DEBUG [NetUtil] - \proc\sys\net\core\somaxconn: 200 (non-existent)
2018/09/13 15:54:25 DEBUG [TransportServer] - Shuffle server started on port: 61591
2018/09/13 15:54:25 INFO  [Utils] - Successfully started service 'sparkDriver' on port 61591.
2018/09/13 15:54:25 DEBUG [SparkEnv] - Using serializer: class org.apache.spark.serializer.KryoSerializer
2018/09/13 15:54:25 INFO  [SparkEnv] - Registering MapOutputTracker
2018/09/13 15:54:25 DEBUG [MapOutputTrackerMasterEndpoint] - init
2018/09/13 15:54:25 INFO  [SparkEnv] - Registering BlockManagerMaster
2018/09/13 15:54:25 INFO  [BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018/09/13 15:54:25 INFO  [BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2018/09/13 15:54:25 INFO  [DiskBlockManager] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-6322b4b4-b15a-4b2b-b364-d558da7e02aa
2018/09/13 15:54:25 DEBUG [DiskBlockManager] - Adding shutdown hook
2018/09/13 15:54:25 DEBUG [ShutdownHookManager] - Adding shutdown hook
2018/09/13 15:54:25 INFO  [MemoryStore] - MemoryStore started with capacity 897.6 MB
2018/09/13 15:54:25 INFO  [SparkEnv] - Registering OutputCommitCoordinator
2018/09/13 15:54:25 DEBUG [OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - init
2018/09/13 15:54:25 DEBUG [SecurityManager] - Created SSL options for ui: SSLOptions{enabled=false, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
2018/09/13 15:54:25 INFO  [Utils] - Successfully started service 'SparkUI' on port 4040.
2018/09/13 15:54:25 INFO  [SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://172.16.91.134:4040
2018/09/13 15:54:25 INFO  [Executor] - Starting executor ID driver on host localhost
2018/09/13 15:54:25 DEBUG [TransportServer] - Shuffle server started on port: 61632
2018/09/13 15:54:25 INFO  [Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61632.
2018/09/13 15:54:25 INFO  [NettyBlockTransferService] - Server created on 172.16.91.134:61632
2018/09/13 15:54:25 INFO  [BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018/09/13 15:54:25 INFO  [BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 172.16.91.134, 61632, None)
2018/09/13 15:54:25 DEBUG [DefaultTopologyMapper] - Got a request for 172.16.91.134
2018/09/13 15:54:25 INFO  [BlockManagerMasterEndpoint] - Registering block manager 172.16.91.134:61632 with 897.6 MB RAM, BlockManagerId(driver, 172.16.91.134, 61632, None)
2018/09/13 15:54:25 INFO  [BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 172.16.91.134, 61632, None)
2018/09/13 15:54:25 INFO  [BlockManager] - Initialized BlockManager: BlockManagerId(driver, 172.16.91.134, 61632, None)
2018/09/13 15:54:25 DEBUG [SparkContext] - Adding shutdown hook
2018/09/13 15:54:25 WARN  [SparkContext] - Using an existing SparkContext; some configuration may not take effect.
2018/09/13 15:54:25 INFO  [SharedState] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IdeaWorkSpace/EnergyManagement4/EnergyManagement/spark-warehouse/').
2018/09/13 15:54:25 INFO  [SharedState] - Warehouse path is 'file:/E:/IdeaWorkSpace/EnergyManagement4/EnergyManagement/spark-warehouse/'.
2018/09/13 15:54:26 INFO  [StateStoreCoordinatorRef] - Registered StateStoreCoordinator endpoint
2018/09/13 15:54:26 ERROR [RiskEvaluationMain$] - ffffff
org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:/Users/魏永朝/Desktop/能源管理平台3期/机泵群风险评估监控场景/data/机泵风险评估数据.csv;
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:360)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:348)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:348)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:533)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:412)
	at com.bonc.examples.RiskEvaluationMain$.main(RiskEvaluationMain.scala:65)
	at com.bonc.examples.RiskEvaluationMain.main(RiskEvaluationMain.scala)
2018/09/13 15:54:26 INFO  [SparkContext] - Invoking stop() from shutdown hook
2018/09/13 15:54:26 INFO  [SparkUI] - Stopped Spark web UI at http://172.16.91.134:4040
2018/09/13 15:54:26 INFO  [MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2018/09/13 15:54:26 INFO  [MemoryStore] - MemoryStore cleared
2018/09/13 15:54:26 INFO  [BlockManager] - BlockManager stopped
2018/09/13 15:54:26 INFO  [BlockManagerMaster] - BlockManagerMaster stopped
2018/09/13 15:54:26 INFO  [OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2018/09/13 15:54:26 INFO  [SparkContext] - Successfully stopped SparkContext
2018/09/13 15:54:26 INFO  [ShutdownHookManager] - Shutdown hook called
2018/09/13 15:54:26 INFO  [ShutdownHookManager] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-66c31c95-8f83-48e1-97eb-b09b7edbbbce
