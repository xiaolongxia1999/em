<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
   <name>hive.execution.engine</name>
   <value>mr</value>
</property>

<property>
   <name>hive.stats.autogather</name>
   <value>true</value>
</property>

<property>
   <name>hive.compute.query.using.stats</name>
   <value>true</value>
</property>

<property>
   <name>hive.mapred.reduce.tasks.speculative.execution</name>
   <value>false</value>
</property>

<property>
   <name>hive.exec.mode.local.auto</name>
   <value>true</value>
   <description>enable local mode</description>
</property>

<property>
   <name>hive.mapred.mode</name>
   <value>nostrict</value>
</property>

<property>
   <name>hive.mapred.supports.subdirectories</name>
   <value>true</value>
</property>

<property>
   <name>hive.exec.script.maxerrsize</name>
   <value>500000</value>
   <description>Default:100000</description>
</property>

<!--mapjoin Start-->
<property>
   <name>hive.auto.convert.join</name>
   <value>true</value>
</property>

<property>
   <name>hive.auto.convert.join.noconditionaltask</name>
   <value>true</value>
</property>

<property>
   <name>hive.auto.convert.join.noconditionaltask.size</name>
   <value>716177408</value>
</property>

<property>
   <name>hive.mapjoin.smalltable.filesize</name>
   <value>25000000</value>
   <description>small table size (bytes)</description>
</property>
<!--mapjoin End-->

<!--hive exec Start-->
<property>
   <name>hive.exec.scratchdir</name>
   <value>/opt/hive/hivescratchdir/hive-${user.name}</value>
</property>

<property>
   <name>hive.exec.parallel</name>
   <value>true</value>
</property>
<property>
   <name>hive.exec.parallel.thread.number</name>
   <value>8</value>
</property>

<property>
   <name>hive.exec.compress.output</name>
   <value>true</value>
</property>

<property>
   <name>hive.exec.dynamic.partition</name>
   <value>true</value>
   <description>enable dynamic partition</description>
</property>

<property>
   <name>hive.exec.dynamic.partition.mode</name>
   <value>strict</value>
   <description>whether or not allow all the partitions are dynamic</description>
</property>

<property>
   <name>hive.exec.max.dynamic.partitions</name>
   <value>1000</value>
</property>

<property>
   <name>hive.exec.max.dynamic.partitions.pernode</name>
   <value>1000</value>
</property>
<!--hive exec Stop-->

<!--sortmerge join Start-->
<property>
   <name>hive.auto.convert.sortmerge.join</name>
   <value>true</value>
</property>

<property>
   <name>hive.smbjoin.cache.rows</name>
   <value>10000</value>
</property>
<!--sortmerge join End-->

<!--ppd Start-->
<property>
   <name>hive.optimize.ppd</name>
   <value>true</value>
</property>

<property>
   <name>hive.optimize.index.filter</name>
   <value>true</value>
</property>
<!--ppd End-->

<!--RSs Start-->    
<property>
   <name>hive.optimize.reducededuplication.min.reducer</name>
   <value>4</value>
</property>

<property>
   <name>hive.limit.pushdown.memory.usage</name>
   <value>0.04</value>
</property>
<!--RSs End-->

<!--Vectorization Start-->
<property>
   <name>hive.vectorized.execution.enabled</name>
   <value>true</value>
</property>

<property>
   <name>hive.vectorized.groupby.flush.percent</name>
   <value>1.0</value>
</property>

<property>
   <name>hive.vectorized.groupby.checkinterval</name>
   <value>1024</value>
</property>

<property>
   <name>hive.vectorized.groupby.maxentries</name>
   <value>1024</value>
</property>
<!--Vectorization End-->

<!--fs disable cache Start-->
<property>
   <name>fs.file.impl.disable.cache</name>
   <value>true</value>
</property>

<property>
   <name>fs.hdfs.impl.disable.cache</name>
   <value>true</value>
</property> 
<!--fs disable cache End-->

<!--hive.sercurity Start-->
<!--
<property>
   <name>hive.security.authorization.enabled</name>
   <value>true</value>
   <final>true</final>
</property>
 <property>
   <name>hive.security.authorization.manager</name>
   <value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider</value>
</property>

<property>
   <name>hive.security.authenticator.manager</name>
   <value>org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator</value>
</property>



<property>
   <name>hive.security.authorization.createtable.owner.grants</name>
   <value>ALL</value>
   <final>true</final>
</property>

<property>
   <name>hive.security.authorization.task.factory</name>
   <value>org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl</value>
   <final>true</final>
</property>
<property>
   <name>hive.security.metastore.authorization.manager</name>
   <value>org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider</value>
</property>

<property>
   <name>hive.exec.pre.hooks</name>
   <value>org.apache.hadoop.hive.ql.hooks.ATSHook</value>
</property>

<property>
   <name>hive.exec.failure.hooks</name>
   <value>org.apache.hadoop.hive.ql.hooks.ATSHook</value>
</property>

<property>
   <name>hive.exec.post.hooks</name>
   <value>org.apache.hadoop.hive.ql.hooks.ATSHook</value>
</property>
-->

<!--
<property>
   <name>hive.semantic.analyzer.hook</name>
   <value>hadoop.hadoop.AuthorityHook</value>
   <final>true</final>
</property>
-->


<!-- 开启hive cli的控制权限 -->  
<property>   
    <name>hive.security.authorization.enabled</name>   
    <value>true</value>   
    <description>enable or disable the hive clientauthorization</description>  
</property>  
<!-- 定义表创建者的权限 -->  
<property>   
    <name>hive.security.authorization.createtable.owner.grants</name>   
    <value>ALL</value>   
    <description>the privileges automatically granted to the owner whenever a table gets created</description>  
</property>  
<!-- 在做类似drop partition操作时，metastore是否要认证权限，默认是false -->  
<property>    
    <name>hive.metastore.authorization.storage.checks</name>    
    <value>true</value>    
</property>  
<!-- 非安全模式，设置为true会令metastore以客户端的用户和组权限执行DFS操作，默认是false，这个属性需要服务端和客户端同时设置 -->  
<property>  
    <name>hive.metastore.execute.setugi</name>  
    <value>false</value>  
</property>    
<!-- 配置超级管理员，需要自定义控制类继承这个AbstractSemanticAnalyzerHook-->  
<property>  
   <name>hive.semantic.analyzer.hook</name>  
   <value>hadoop.hadoop.AuthorityHook</value>  
</property>  

<property>
  <name>hive.semantic.analyzer.hook.admin</name>
  <value>hadoop</value>
</property>  
<!-- 假如出现以下错误：  
          Error while compiling statement: FAILED: SemanticException The current builtin authorization in Hive is incomplete and disabled.  
     需要配置下面的属性 -->  
<property>  
    <name>hive.security.authorization.task.factory</name>  
    <value>org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl</value>  
</property>  




<!--hive.sercurity End-->

<!--metastore Start--> 

<property>
   <name>hive.metastore.uris</name>
   <value>thrift://hadoop01:9083,thrift://hadoop02:9083</value>
</property>

<property>
   <name>hive.metastore.warehouse.dir</name>
   <value>/user/hive/warehouse</value>
</property>


<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://hadoop02/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8</value>
</property>

<property>
   <name>javax.jdo.option.ConnectionDriverName</name>
   <value>com.mysql.jdbc.Driver</value>
</property>

<property>
   <name>javax.jdo.option.ConnectionUserName</name>
   <value>hive</value>
</property>

<property>
   <name>javax.jdo.option.ConnectionPassword</name>
   <value>hive</value>
</property>


<property>
   <name>hive.metastore.client.socket.timeout</name>
   <value>60</value>
</property>

<property>
   <name>hive.metastore.cache.pinobjtypes</name>
   <value>Table,Database,Type,FieldSchema,Order</value>
   <description>Default:Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order</description>
</property>

<property>
   <name>hive.metastore.execute.setugi</name>
   <value>true</value>
</property>

<property>
   <name>hive.metastore.partition.inherit.table.properties</name>
   <value>*</value>
</property>

<property>
   <name>hive.metastore.authorization.storage.checks</name>
   <value>true</value>
</property>

<!--metastore End-->   

<!--hwi Start-->
<!--
     <property>
   <name>hive.hwi.listen.host</name>
   <value>fengw01</value>
   <description>This is the host address the Hive Web Interface will listen on</description>
</property>

<property>
   <name>hive.hwi.listen.port</name>
   <value>9999</value>
   <description>This is the port the Hive Web Interface will listen on</description>
</property>

<property>
   <name>hive.hwi.war.file</name>
   <value>lib/hive-hwi-1.2.1.jar</value>
   <description>This is the WAR file with the jsp content for Hive Web Interface</description>
</property>
-->
<!--hwi End-->

<!--hiveserver End-->
<!--hiveservr2 Start-->
<property>
   <name>hive.server2.thrift.port</name>
   <value>14000</value>
</property>

<property>
   <name>hive.server2.thrift.bind.host</name>
   <value>hadoop02</value>
</property>  

<property>
   <name>hive.server2.enable.doAs</name>
   <value>false</value>
</property>

<property>
   <name>hive.server2.thrift.min.worker.threads</name>
   <value>20</value>
</property>

<property>
   <name>hive.server2.thrift.max.worker.threads</name>
   <value>500</value>
</property>

<property>
   <name>hive.server2.thrift.worker.keepalive.time</name>
   <value>60</value>
</property>

<property>
   <name>hive.server2.transport.mode</name>
   <value>binary</value>
</property>
<!--hiveserver2 End-->

<!--concurrency Start-->
<property>
   <name>hive.support.concurrency</name>
   <description>Enable Hive's Table Lock Manager Service</description>
   <value>true</value>
</property>  

<property>
   <name>hive.zookeeper.quorum</name>
   <description>Zookeeper quorum used by Hive's Table Lock Manager</description>
   <value>hadoop03,hadoop04,hadoop05</value>
</property> 

<property>
   <name>hive.zookeeper.client.port</name>
   <value>2181</value>
</property>
<!--concurrency End-->

<!--txn Start-->
<!--
     <property>
   <name>hive.enforce.bucketing</name>
   <value>true</value>
</property>

<property>
   <name>hive.enforce.sorting</name>
   <value>true</value>
</property>

<property>
   <name>hive.enforce.sortmergebucketmapjoin</name>
   <value>true</value>
</property>

<property>
   <name>hive.txn.manager</name>
   <value>org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager</value>
</property>

<property>
   <name>hive.txn.max.open.batch</name>
   <value>1000</value>
</property>

<property>
   <name>hive.txn.timeout</name>
   <value>300</value>
</property>
-->
<!--txn End-->
<!--txn compactor Start-->
<!--
     <property>
   <name>hive.compactor.initiator.on</name>
   <value>false</value>
</property>

<property>
   <name>hive.compactor.worker.threads</name>
   <value>0</value>
</property>

<property>
   <name>hive.compactor.worker.timeout</name>
   <value>86400L</value>
</property>

<property>
   <name>hive.compactor.abortedtxn.threshold</name>
   <value>1000</value>
</property>

<property>
   <name>hive.compactor.delta.num.threshold</name>
   <value>10</value>
</property>

<property>
   <name>hive.compactor.check.interval</name>
   <value>300</value>
</property>

<property>
   <name>hive.compactor.delta.pct.threshold</name>
   <value>0.1f</value>
</property>    
-->
<!--txn compactor End-->

<!--tez Start-->
<!--
<property>
   <name>hive.tez.container.size</name>
   <value>2048</value>
</property>

<property>
   <name>hive.tez.java.opts</name>
   <value>-server -Xmx2048m -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC</value>
</property>
-->
<!--
<property>
   <name>hive.server2.tez.initialize.default.sessions</name>
   <value>false</value>
</property>

<property>
   <name>hive.server2.tez.default.queues</name>
   <value>default</value>
</property>

<property>
   <name>hive.tez.input.format</name>
   <value>org.apache.hadoop.hive.ql.io.HiveInputFormat</value>
</property>

<property>
   <name>hive.server2.tez.sessions.per.default.queue</name>
   <value>1</value>
</property>
-->
<!--tez End-->

<!--hive cli Start-->
 <property>
   <name>hive.cli.print.header</name>
   <value>true</value>
</property>
<property>
   <name>hive.cli.print.current.db</name>
   <value>true</value>
</property>
<!--hive cli End-->
<!--
<property>
   <name>hive.aux.jars.path</name>
   <value>file:///opt/beh/core/hive/lib/hive-hbase-handler-1.1.0-cdh5.7.0.jar,file:///opt/beh/core/hbase/lib/protobuf-java-2.5.0.jar,file:///opt/beh/core/hbase/lib/hbase-client-1.2.0-cdh5.7.0.jar,file:///opt/beh/core/hbase/lib/hbase-common-1.2.0-cdh5.7.0.jar,file:///opt/beh/core/zookeeper/zookeeper-3.4.5-cdh5.7.0.jar,file:///opt/beh/core/hive/lib/guava-14.0.1.jar</value>
</property>
-->
<property>
   <name>mapreduce.client.genericoptionsparser.used</name>
   <value>true</value>
</property>


  <property>
    <name>hive.hwi.listen.host</name>
    <value>hadoop02</value>
    <description>This is the host address the Hive Web Interface will listen on</description>
  </property>

  <property>
    <name>hive.hwi.listen.port</name>
    <value>19999</value>
    <description>This is the port the Hive Web Interface will listen on</description>
  </property>

  <property>
    <name>hive.hwi.war.file</name>
    <value>lib/hive-hwi-0.12.0.war</value>
    <description>This is the WAR file with the jsp content for Hive Web Interface</description>
  </property>
</configuration>

